{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "908ef4d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.10 environment at: /teamspace/studios/this_studio/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 82ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install 'transformers[torch]' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d08bdef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2mUsing Python 3.10.10 environment at: /teamspace/studios/this_studio/.venv\u001b[0m\n",
      "\u001b[2mAudited \u001b[1m1 package\u001b[0m \u001b[2min 2ms\u001b[0m\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%uv pip install torchinfo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb1b263",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/teamspace/studios/this_studio/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 2.6.0+cu124\n",
      "Transformers version: 4.51.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)\n",
    "print(\"Transformers version:\", transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a1ec5b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "======================================================================\n",
       "Layer (type:depth-idx)                        Param #\n",
       "======================================================================\n",
       "GPT2Model                                     --\n",
       "├─Embedding: 1-1                              38,597,376\n",
       "├─Embedding: 1-2                              786,432\n",
       "├─Dropout: 1-3                                --\n",
       "├─ModuleList: 1-4                             --\n",
       "│    └─GPT2Block: 2-1                         --\n",
       "│    │    └─LayerNorm: 3-1                    1,536\n",
       "│    │    └─GPT2Attention: 3-2                2,362,368\n",
       "│    │    └─LayerNorm: 3-3                    1,536\n",
       "│    │    └─GPT2MLP: 3-4                      4,722,432\n",
       "│    └─GPT2Block: 2-2                         --\n",
       "│    │    └─LayerNorm: 3-5                    1,536\n",
       "│    │    └─GPT2Attention: 3-6                2,362,368\n",
       "│    │    └─LayerNorm: 3-7                    1,536\n",
       "│    │    └─GPT2MLP: 3-8                      4,722,432\n",
       "│    └─GPT2Block: 2-3                         --\n",
       "│    │    └─LayerNorm: 3-9                    1,536\n",
       "│    │    └─GPT2Attention: 3-10               2,362,368\n",
       "│    │    └─LayerNorm: 3-11                   1,536\n",
       "│    │    └─GPT2MLP: 3-12                     4,722,432\n",
       "│    └─GPT2Block: 2-4                         --\n",
       "│    │    └─LayerNorm: 3-13                   1,536\n",
       "│    │    └─GPT2Attention: 3-14               2,362,368\n",
       "│    │    └─LayerNorm: 3-15                   1,536\n",
       "│    │    └─GPT2MLP: 3-16                     4,722,432\n",
       "│    └─GPT2Block: 2-5                         --\n",
       "│    │    └─LayerNorm: 3-17                   1,536\n",
       "│    │    └─GPT2Attention: 3-18               2,362,368\n",
       "│    │    └─LayerNorm: 3-19                   1,536\n",
       "│    │    └─GPT2MLP: 3-20                     4,722,432\n",
       "│    └─GPT2Block: 2-6                         --\n",
       "│    │    └─LayerNorm: 3-21                   1,536\n",
       "│    │    └─GPT2Attention: 3-22               2,362,368\n",
       "│    │    └─LayerNorm: 3-23                   1,536\n",
       "│    │    └─GPT2MLP: 3-24                     4,722,432\n",
       "│    └─GPT2Block: 2-7                         --\n",
       "│    │    └─LayerNorm: 3-25                   1,536\n",
       "│    │    └─GPT2Attention: 3-26               2,362,368\n",
       "│    │    └─LayerNorm: 3-27                   1,536\n",
       "│    │    └─GPT2MLP: 3-28                     4,722,432\n",
       "│    └─GPT2Block: 2-8                         --\n",
       "│    │    └─LayerNorm: 3-29                   1,536\n",
       "│    │    └─GPT2Attention: 3-30               2,362,368\n",
       "│    │    └─LayerNorm: 3-31                   1,536\n",
       "│    │    └─GPT2MLP: 3-32                     4,722,432\n",
       "│    └─GPT2Block: 2-9                         --\n",
       "│    │    └─LayerNorm: 3-33                   1,536\n",
       "│    │    └─GPT2Attention: 3-34               2,362,368\n",
       "│    │    └─LayerNorm: 3-35                   1,536\n",
       "│    │    └─GPT2MLP: 3-36                     4,722,432\n",
       "│    └─GPT2Block: 2-10                        --\n",
       "│    │    └─LayerNorm: 3-37                   1,536\n",
       "│    │    └─GPT2Attention: 3-38               2,362,368\n",
       "│    │    └─LayerNorm: 3-39                   1,536\n",
       "│    │    └─GPT2MLP: 3-40                     4,722,432\n",
       "│    └─GPT2Block: 2-11                        --\n",
       "│    │    └─LayerNorm: 3-41                   1,536\n",
       "│    │    └─GPT2Attention: 3-42               2,362,368\n",
       "│    │    └─LayerNorm: 3-43                   1,536\n",
       "│    │    └─GPT2MLP: 3-44                     4,722,432\n",
       "│    └─GPT2Block: 2-12                        --\n",
       "│    │    └─LayerNorm: 3-45                   1,536\n",
       "│    │    └─GPT2Attention: 3-46               2,362,368\n",
       "│    │    └─LayerNorm: 3-47                   1,536\n",
       "│    │    └─GPT2MLP: 3-48                     4,722,432\n",
       "├─LayerNorm: 1-5                              1,536\n",
       "======================================================================\n",
       "Total params: 124,439,808\n",
       "Trainable params: 124,439,808\n",
       "Non-trainable params: 0\n",
       "======================================================================"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "from transformers import GPT2Tokenizer, GPT2Model\n",
    "\n",
    "model_hf = GPT2Model.from_pretrained('gpt2')\n",
    "\n",
    "weights = model_hf.state_dict()\n",
    "\n",
    "summary(model_hf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d47e3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wte.weight torch.Size([50257, 768])\n",
      "wpe.weight torch.Size([1024, 768])\n",
      "h.0.ln_1.weight torch.Size([768])\n",
      "h.0.ln_1.bias torch.Size([768])\n",
      "h.0.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.0.attn.c_attn.bias torch.Size([2304])\n",
      "h.0.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.0.attn.c_proj.bias torch.Size([768])\n",
      "h.0.ln_2.weight torch.Size([768])\n",
      "h.0.ln_2.bias torch.Size([768])\n",
      "h.0.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.0.mlp.c_fc.bias torch.Size([3072])\n",
      "h.0.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.0.mlp.c_proj.bias torch.Size([768])\n",
      "h.1.ln_1.weight torch.Size([768])\n",
      "h.1.ln_1.bias torch.Size([768])\n",
      "h.1.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.1.attn.c_attn.bias torch.Size([2304])\n",
      "h.1.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.1.attn.c_proj.bias torch.Size([768])\n",
      "h.1.ln_2.weight torch.Size([768])\n",
      "h.1.ln_2.bias torch.Size([768])\n",
      "h.1.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.1.mlp.c_fc.bias torch.Size([3072])\n",
      "h.1.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.1.mlp.c_proj.bias torch.Size([768])\n",
      "h.2.ln_1.weight torch.Size([768])\n",
      "h.2.ln_1.bias torch.Size([768])\n",
      "h.2.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.2.attn.c_attn.bias torch.Size([2304])\n",
      "h.2.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.2.attn.c_proj.bias torch.Size([768])\n",
      "h.2.ln_2.weight torch.Size([768])\n",
      "h.2.ln_2.bias torch.Size([768])\n",
      "h.2.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.2.mlp.c_fc.bias torch.Size([3072])\n",
      "h.2.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.2.mlp.c_proj.bias torch.Size([768])\n",
      "h.3.ln_1.weight torch.Size([768])\n",
      "h.3.ln_1.bias torch.Size([768])\n",
      "h.3.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.3.attn.c_attn.bias torch.Size([2304])\n",
      "h.3.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.3.attn.c_proj.bias torch.Size([768])\n",
      "h.3.ln_2.weight torch.Size([768])\n",
      "h.3.ln_2.bias torch.Size([768])\n",
      "h.3.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.3.mlp.c_fc.bias torch.Size([3072])\n",
      "h.3.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.3.mlp.c_proj.bias torch.Size([768])\n",
      "h.4.ln_1.weight torch.Size([768])\n",
      "h.4.ln_1.bias torch.Size([768])\n",
      "h.4.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.4.attn.c_attn.bias torch.Size([2304])\n",
      "h.4.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.4.attn.c_proj.bias torch.Size([768])\n",
      "h.4.ln_2.weight torch.Size([768])\n",
      "h.4.ln_2.bias torch.Size([768])\n",
      "h.4.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.4.mlp.c_fc.bias torch.Size([3072])\n",
      "h.4.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.4.mlp.c_proj.bias torch.Size([768])\n",
      "h.5.ln_1.weight torch.Size([768])\n",
      "h.5.ln_1.bias torch.Size([768])\n",
      "h.5.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.5.attn.c_attn.bias torch.Size([2304])\n",
      "h.5.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.5.attn.c_proj.bias torch.Size([768])\n",
      "h.5.ln_2.weight torch.Size([768])\n",
      "h.5.ln_2.bias torch.Size([768])\n",
      "h.5.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.5.mlp.c_fc.bias torch.Size([3072])\n",
      "h.5.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.5.mlp.c_proj.bias torch.Size([768])\n",
      "h.6.ln_1.weight torch.Size([768])\n",
      "h.6.ln_1.bias torch.Size([768])\n",
      "h.6.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.6.attn.c_attn.bias torch.Size([2304])\n",
      "h.6.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.6.attn.c_proj.bias torch.Size([768])\n",
      "h.6.ln_2.weight torch.Size([768])\n",
      "h.6.ln_2.bias torch.Size([768])\n",
      "h.6.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.6.mlp.c_fc.bias torch.Size([3072])\n",
      "h.6.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.6.mlp.c_proj.bias torch.Size([768])\n",
      "h.7.ln_1.weight torch.Size([768])\n",
      "h.7.ln_1.bias torch.Size([768])\n",
      "h.7.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.7.attn.c_attn.bias torch.Size([2304])\n",
      "h.7.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.7.attn.c_proj.bias torch.Size([768])\n",
      "h.7.ln_2.weight torch.Size([768])\n",
      "h.7.ln_2.bias torch.Size([768])\n",
      "h.7.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.7.mlp.c_fc.bias torch.Size([3072])\n",
      "h.7.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.7.mlp.c_proj.bias torch.Size([768])\n",
      "h.8.ln_1.weight torch.Size([768])\n",
      "h.8.ln_1.bias torch.Size([768])\n",
      "h.8.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.8.attn.c_attn.bias torch.Size([2304])\n",
      "h.8.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.8.attn.c_proj.bias torch.Size([768])\n",
      "h.8.ln_2.weight torch.Size([768])\n",
      "h.8.ln_2.bias torch.Size([768])\n",
      "h.8.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.8.mlp.c_fc.bias torch.Size([3072])\n",
      "h.8.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.8.mlp.c_proj.bias torch.Size([768])\n",
      "h.9.ln_1.weight torch.Size([768])\n",
      "h.9.ln_1.bias torch.Size([768])\n",
      "h.9.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.9.attn.c_attn.bias torch.Size([2304])\n",
      "h.9.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.9.attn.c_proj.bias torch.Size([768])\n",
      "h.9.ln_2.weight torch.Size([768])\n",
      "h.9.ln_2.bias torch.Size([768])\n",
      "h.9.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.9.mlp.c_fc.bias torch.Size([3072])\n",
      "h.9.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.9.mlp.c_proj.bias torch.Size([768])\n",
      "h.10.ln_1.weight torch.Size([768])\n",
      "h.10.ln_1.bias torch.Size([768])\n",
      "h.10.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.10.attn.c_attn.bias torch.Size([2304])\n",
      "h.10.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.10.attn.c_proj.bias torch.Size([768])\n",
      "h.10.ln_2.weight torch.Size([768])\n",
      "h.10.ln_2.bias torch.Size([768])\n",
      "h.10.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.10.mlp.c_fc.bias torch.Size([3072])\n",
      "h.10.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.10.mlp.c_proj.bias torch.Size([768])\n",
      "h.11.ln_1.weight torch.Size([768])\n",
      "h.11.ln_1.bias torch.Size([768])\n",
      "h.11.attn.c_attn.weight torch.Size([768, 2304])\n",
      "h.11.attn.c_attn.bias torch.Size([2304])\n",
      "h.11.attn.c_proj.weight torch.Size([768, 768])\n",
      "h.11.attn.c_proj.bias torch.Size([768])\n",
      "h.11.ln_2.weight torch.Size([768])\n",
      "h.11.ln_2.bias torch.Size([768])\n",
      "h.11.mlp.c_fc.weight torch.Size([768, 3072])\n",
      "h.11.mlp.c_fc.bias torch.Size([3072])\n",
      "h.11.mlp.c_proj.weight torch.Size([3072, 768])\n",
      "h.11.mlp.c_proj.bias torch.Size([768])\n",
      "ln_f.weight torch.Size([768])\n",
      "ln_f.bias torch.Size([768])\n"
     ]
    }
   ],
   "source": [
    "for key, shape in model_hf.state_dict().items():\n",
    "    \n",
    "    print(key, shape.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b283573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1.8821e-02, -1.9742e-01,  4.0267e-03,  ..., -4.3044e-02,\n",
       "           2.8267e-02,  5.4490e-02],\n",
       "         [ 2.3959e-02, -5.3792e-02, -9.4879e-02,  ...,  3.4170e-02,\n",
       "           1.0172e-02, -1.5573e-04],\n",
       "         [ 4.2161e-03, -8.4764e-02,  5.4515e-02,  ...,  1.9745e-02,\n",
       "           1.9325e-02, -2.1424e-02],\n",
       "         ...,\n",
       "         [-1.7987e-03,  1.6052e-03, -5.5103e-02,  ...,  1.3617e-02,\n",
       "          -7.1805e-03,  3.7552e-03],\n",
       "         [ 3.2105e-03,  1.5501e-03, -4.8944e-02,  ...,  2.0725e-02,\n",
       "          -1.1838e-02, -5.5683e-04],\n",
       "         [ 2.6610e-04,  3.0272e-03, -1.7086e-03,  ..., -4.6506e-03,\n",
       "          -2.3541e-03, -5.7855e-03]]),\n",
       " torch.Size([1024, 768]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['wpe.weight'] , weights['wpe.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c4c239e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{2: 3}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict({2:3})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad27f5cb",
   "metadata": {},
   "source": [
    "#### Working with Tinyshakespeare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b9176ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You\n"
     ]
    }
   ],
   "source": [
    "# !wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
    "with open('input.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(text[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a9aa6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5962, 22307, 25, 198, 8421, 356, 5120, 597, 2252, 11, 3285, 502, 2740, 13, 198, 198, 3237, 25, 198, 5248, 461, 11, 2740, 13, 198, 198, 5962, 22307, 25, 198, 1639, 389, 477, 12939, 2138, 284, 4656, 621, 284, 1145, 680, 30, 198, 198, 3237, 25, 198, 4965, 5634, 13, 12939, 13, 198, 198, 5962, 22307, 25, 198, 5962, 11, 345, 760, 327, 1872, 385, 1526, 28599, 318, 4039, 4472, 284, 262, 661, 13, 198, 198, 3237, 25, 198, 1135, 760, 470, 11, 356, 760, 470, 13, 198, 198, 5962, 22307, 25, 198, 5756, 514, 1494, 683, 11, 290, 356]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "encoding = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = encoding.encode(text)\n",
    "print(tokens[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ea1f2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to put this in a way of (Batch, sequence_length)\n",
    "import torch\n",
    "\n",
    "buff = torch.tensor(tokens[:24])\n",
    "x = buff.view(4,6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0305b30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 5962, 22307,    25,   198,  8421,   356],\n",
       "         [ 5120,   597,  2252,    11,  3285,   502],\n",
       "         [ 2740,    13,   198,   198,  3237,    25],\n",
       "         [  198,  5248,   461,    11,  2740,    13]]),\n",
       " tensor([[22307,    25,   198,  8421,   356,  5120],\n",
       "         [  597,  2252,    11,  3285,   502,  2740],\n",
       "         [   13,   198,   198,  3237,    25,   198],\n",
       "         [ 5248,   461,    11,  2740,    13,   198]]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting the targets\n",
    "buff = torch.tensor(tokens[:24+1])\n",
    "x = buff[:-1].view(4,6)\n",
    "y = buff[1:].view(4,6)\n",
    "x,y #now we have labels for each one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0f5df439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "True if 'lm_head' in weights.keys() else False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "249c7184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([768]), torch.Size([50257, 768]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights['ln_f.weight'].shape, weights['wte.weight'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e607b195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(9.5927)\n"
     ]
    }
   ],
   "source": [
    "#This eg shows how adding of random weights can lead to the overflow of values\n",
    "#why we need the layer normalization to control it\n",
    "# This example shows visually how repeated additions can cause a vector's \"noise\" or variability to grow significantly, making normalization crucial in deep neural networks.\n",
    "\n",
    "\n",
    "import torch\n",
    "x = torch.zeros(768)\n",
    "\n",
    "for i in range(100): #100 layers\n",
    "    x += torch.randn(768) \n",
    "\n",
    "print(x.std()) #how spreaded your values are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752cf6c1",
   "metadata": {},
   "source": [
    "This shows that as we keep on adding the values . Our values get keep on growing due to which it becomes unstable\n",
    "to train . \n",
    "\n",
    "-> Thats when we use layer normalization to stabilize the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f237a9d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.2071)\n",
      "tensor(0.2085)\n",
      "tensor(1.0000)\n"
     ]
    }
   ],
   "source": [
    "#so above you could see that the values were exploded\n",
    "#buf if we normalize it with standard deviation\n",
    "\n",
    "\n",
    "# A. Standard Deviation\n",
    "x = torch.zeros(768)\n",
    "for i in range(100): #100 layers\n",
    "    x += torch.randn(768) * 0.02 \n",
    "\n",
    "print(x.std()) #how spreaded your values are\n",
    "# (✓100 * 0.02) = 0.2\n",
    "#so now the values will be more controlled\n",
    "\n",
    "\n",
    "# B. Residual Connection (skip connections)\n",
    "x = torch.zeros(768)\n",
    "for _ in range(100):\n",
    "    x = x + (torch.randn(768) * 0.02)  # Residual: x = x + F(x)\n",
    "print(x.std())  # Output: ~0.2 (controlled)\n",
    "\n",
    "\n",
    "# C. Layer Normalization\n",
    "x = torch.zeros(768)\n",
    "for _ in range(100):\n",
    "    x = x + torch.randn(768) * 0.02\n",
    "    x = (x - x.mean()) / (x.std() + 1e-5)  # Normalize\n",
    "print(x.std())  # Output: ~1.0 (forced to unit variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22607495",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9559)\n"
     ]
    }
   ],
   "source": [
    "#we can control the scaling either this way\n",
    "\n",
    "x = torch.zeros(768)\n",
    "n = 100\n",
    "for i in range(100): #100 layers\n",
    "    x += n ** -0.5 * torch.randn(768)\n",
    "\n",
    "print(x.std()) #how spreaded your values are"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f30869",
   "metadata": {},
   "source": [
    "Let's explain this clearly, step-by-step, to understand why scaling by \\(1/\\sqrt{n}\\) works:\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 1: Understanding without scaling**\n",
    "\n",
    "Suppose you add random noise to a vector repeatedly (without scaling):\n",
    "\n",
    "```python\n",
    "x = torch.zeros(768)\n",
    "for i in range(n):\n",
    "    x += torch.randn(768)\n",
    "```\n",
    "\n",
    "**What happens?**  \n",
    "Each random vector you add has a standard deviation of about **1**.  \n",
    "When you add these vectors repeatedly, the variances **add up**.\n",
    "\n",
    "- Variance after 1 addition: **1**\n",
    "- Variance after 2 additions: **2**\n",
    "- Variance after \\( n \\) additions: **n**\n",
    "\n",
    "**Standard deviation** = √(variance)  \n",
    "Thus, after adding random noise \\( n \\) times, the standard deviation grows to:\n",
    "\n",
    "\\[\n",
    "\\sqrt{n}\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Why scaling by \\(1/\\sqrt{n}\\)?**\n",
    "\n",
    "You scaled each addition like this:\n",
    "\n",
    "```python\n",
    "x += torch.randn(768) / sqrt(n)\n",
    "```\n",
    "\n",
    "Now, each added vector has standard deviation:\n",
    "\n",
    "\\[\n",
    "\\frac{1}{\\sqrt{n}}\n",
    "\\]\n",
    "\n",
    "**Let's find the variance clearly:**  \n",
    "- Each scaled addition now has **variance** \\( (1/\\sqrt{n})^2 = 1/n \\).\n",
    "- After adding these scaled vectors \\( n \\) times, the total variance is:\n",
    "\n",
    "\\[\n",
    "\\text{variance after n additions} = n \\times \\frac{1}{n} = 1\n",
    "\\]\n",
    "\n",
    "Thus, the variance remains constant (**1**) even after \\( n \\) additions.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: The intuitive reason why it works**\n",
    "\n",
    "- **Without scaling:** Each addition makes the spread (standard deviation) bigger, because you keep adding the same-sized noise.\n",
    "- **With scaling:** You're adding smaller noise each time, exactly chosen so that the total spread stays the same.\n",
    "\n",
    "You choose exactly **\\(1/\\sqrt{n}\\)** because variance grows linearly (by addition), and standard deviation grows by the square root. Thus, dividing by **√n** precisely compensates for the growth that would otherwise happen.\n",
    "\n",
    "---\n",
    "\n",
    "### **Simple summary:**\n",
    "- Without scaling → The spread grows as you add more random vectors.\n",
    "- With scaling by \\(1/\\sqrt{n}\\) → The spread remains stable, at about 1.\n",
    "\n",
    "This scaling trick is crucial to keep numerical stability in neural network computations, ensuring consistent behavior across layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3415e45b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 ** -0.5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
